{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65ba078-bdd2-4066-9012-960d857ef093",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \u001b[38;5;66;03m# Explicitly import numpy\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Azure ML SDK imports for deployment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazureml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workspace, Model, Environment\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazureml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconda_dependencies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CondaDependencies \u001b[38;5;66;03m# Used internally by Environment.from_conda_specification\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazureml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceConfig\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azureml'"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Import Libraries ---\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from azure.cosmos import CosmosClient, exceptions # For synchronous Cosmos DB client\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np # Explicitly import numpy\n",
    "\n",
    "# Azure ML SDK imports for deployment\n",
    "from azureml.core import Workspace, Model, Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies # Used internally by Environment.from_conda_specification\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication # For interactive login if needed\n",
    "\n",
    "print(\"All required libraries imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c16743-5536-4871-82f8-39ba0e6bfd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosmos DB client and container initialized for batch query.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Load Environment Variables & Initialize Cosmos DB Client ---\n",
    "# Load environment variables from .env file (ensure .env is in the same directory as this notebook)\n",
    "load_dotenv() \n",
    "\n",
    "# --- Cosmos DB Configuration ---\n",
    "COSMOS_DB_URI = os.getenv(\"COSMOS_DB_URI\")\n",
    "COSMOS_DB_KEY = os.getenv(\"COSMOS_DB_KEY\")\n",
    "COSMOS_DB_DATABASE_ID = \"iot-sensor-db\"\n",
    "COSMOS_DB_CONTAINER_ID = \"anomalies\"\n",
    "\n",
    "# --- Initialize Cosmos DB client (synchronous version for batch query) ---\n",
    "cosmos_client = None\n",
    "try:\n",
    "    if COSMOS_DB_URI and COSMOS_DB_KEY:\n",
    "        cosmos_client = CosmosClient(COSMOS_DB_URI, credential=COSMOS_DB_KEY)\n",
    "        database = cosmos_client.get_database_client(COSMOS_DB_DATABASE_ID)\n",
    "        container = database.get_container_client(COSMOS_DB_CONTAINER_ID)\n",
    "        print(\"Cosmos DB client and container initialized for batch query.\")\n",
    "    else:\n",
    "        print(\"ERROR: Cosmos DB credentials not found. Cannot connect for batch query. Check .env file.\")\n",
    "except exceptions.CosmosResourceNotFoundError as e:\n",
    "    print(f\"ERROR: Cosmos DB resource not found: {e}. Please ensure database '{COSMOS_DB_DATABASE_ID}' and container '{COSMOS_DB_CONTAINER_ID}' exist and names are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize Cosmos DB client for batch query: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09e456f-ce98-4c5a-ae99-297e25606602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 700 records from Cosmos DB for training.\n",
      "DataFrame loaded with shape: (700, 26)\n",
      "\n",
      "First 5 rows of the normal data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_number</th>\n",
       "      <th>time_in_cycles</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "      <th>setting_1</th>\n",
       "      <th>setting_2</th>\n",
       "      <th>setting_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.36</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.75</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit_number  time_in_cycles  sensor_1  sensor_2  sensor_3  sensor_4  \\\n",
       "0            1               1       NaN       NaN       NaN       NaN   \n",
       "1            1               1    518.67    641.82   1589.70   1400.60   \n",
       "2            1               2       NaN       NaN       NaN       NaN   \n",
       "3            1               2    518.67    642.15   1591.82   1403.14   \n",
       "4            1               3       NaN       NaN       NaN       NaN   \n",
       "\n",
       "   sensor_5  sensor_6  sensor_7  sensor_8  ...  sensor_15  sensor_16  \\\n",
       "0       NaN       NaN       NaN       NaN  ...        NaN        NaN   \n",
       "1     14.62     21.61    554.36   2388.06  ...     8.4195       0.03   \n",
       "2       NaN       NaN       NaN       NaN  ...        NaN        NaN   \n",
       "3     14.62     21.61    553.75   2388.04  ...     8.4318       0.03   \n",
       "4       NaN       NaN       NaN       NaN  ...        NaN        NaN   \n",
       "\n",
       "   sensor_17  sensor_18  sensor_19  sensor_20  sensor_21  setting_1  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1      392.0     2388.0      100.0      39.06    23.4190    -0.0007   \n",
       "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3      392.0     2388.0      100.0      39.00    23.4236     0.0019   \n",
       "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "   setting_2  setting_3  \n",
       "0        NaN        NaN  \n",
       "1    -0.0004      100.0  \n",
       "2        NaN        NaN  \n",
       "3    -0.0003      100.0  \n",
       "4        NaN        NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info (data types and non-null counts):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700 entries, 0 to 699\n",
      "Data columns (total 26 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   unit_number     700 non-null    int64  \n",
      " 1   time_in_cycles  700 non-null    int64  \n",
      " 2   sensor_1        250 non-null    float64\n",
      " 3   sensor_2        250 non-null    float64\n",
      " 4   sensor_3        250 non-null    float64\n",
      " 5   sensor_4        250 non-null    float64\n",
      " 6   sensor_5        250 non-null    float64\n",
      " 7   sensor_6        250 non-null    float64\n",
      " 8   sensor_7        250 non-null    float64\n",
      " 9   sensor_8        250 non-null    float64\n",
      " 10  sensor_9        250 non-null    float64\n",
      " 11  sensor_10       250 non-null    float64\n",
      " 12  sensor_11       250 non-null    float64\n",
      " 13  sensor_12       250 non-null    float64\n",
      " 14  sensor_13       250 non-null    float64\n",
      " 15  sensor_14       250 non-null    float64\n",
      " 16  sensor_15       250 non-null    float64\n",
      " 17  sensor_16       250 non-null    float64\n",
      " 18  sensor_17       250 non-null    float64\n",
      " 19  sensor_18       250 non-null    float64\n",
      " 20  sensor_19       250 non-null    float64\n",
      " 21  sensor_20       250 non-null    float64\n",
      " 22  sensor_21       250 non-null    float64\n",
      " 23  setting_1       250 non-null    float64\n",
      " 24  setting_2       250 non-null    float64\n",
      " 25  setting_3       250 non-null    float64\n",
      "dtypes: float64(24), int64(2)\n",
      "memory usage: 142.3 KB\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Query & Load Data for Training ---\n",
    "# Define the query to get \"normal\" data for training.\n",
    "# We'll fetch all sensor measurements (sensor_1 to sensor_21) and operational settings (setting_1 to setting_3).\n",
    "# We'll query for units 1.0 to 5.0 and cycles <= 50, as these are typically considered healthy/normal.\n",
    "# Ensure these match the fields you are actually storing in Cosmos DB.\n",
    "query = \"\"\"\n",
    "SELECT c.unit_number, c.time_in_cycles, \n",
    "       c.setting_1, c.setting_2, c.setting_3,\n",
    "       c.sensor_1, c.sensor_2, c.sensor_3, c.sensor_4, c.sensor_5, c.sensor_6, c.sensor_7, c.sensor_8, c.sensor_9, \n",
    "       c.sensor_10, c.sensor_11, c.sensor_12, c.sensor_13, c.sensor_14, c.sensor_15, c.sensor_16, c.sensor_17, \n",
    "       c.sensor_18, c.sensor_19, c.sensor_20, c.sensor_21\n",
    "FROM c \n",
    "WHERE c.time_in_cycles <= 50 AND c.unit_number IN (1.0, 2.0, 3.0, 4.0, 5.0)\n",
    "ORDER BY c.unit_number ASC, c.time_in_cycles ASC\n",
    "\"\"\"\n",
    "\n",
    "items = []\n",
    "if 'container' in locals() and container is not None: # Check if container was successfully initialized\n",
    "    try:\n",
    "        # enable_cross_partition_query=True is needed for queries across multiple partition key values\n",
    "        for item in container.query_items(query=query, enable_cross_partition_query=True):\n",
    "            items.append(item)\n",
    "        print(f\"Retrieved {len(items)} records from Cosmos DB for training.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error querying Cosmos DB: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: Cosmos DB container not available for querying. Check previous cell output.\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "if items:\n",
    "    df_normal = pd.DataFrame(items)\n",
    "    print(f\"DataFrame loaded with shape: {df_normal.shape}\")\n",
    "    print(\"\\nFirst 5 rows of the normal data:\")\n",
    "    display(df_normal.head()) \n",
    "    \n",
    "    print(\"\\nDataFrame Info (data types and non-null counts):\")\n",
    "    df_normal.info()\n",
    "else:\n",
    "    df_normal = pd.DataFrame()\n",
    "    print(\"No data retrieved to create DataFrame. df_normal is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a9d84-b399-4d56-ba4a-ca957d3407a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Cell 4: Feature Selection & Data Cleaning ---\n",
    "# Identify feature columns (sensor measurements + operational settings)\n",
    "# These names must exactly match the columns in your Cosmos DB documents and original data.\n",
    "feature_cols = ['setting_1', 'setting_2', 'setting_3'] + \\\n",
    "               [f'sensor_{i}' for i in range(1, 22)] \n",
    "\n",
    "# Ensure all feature_cols exist in your DataFrame. This is critical for model consistency.\n",
    "missing_cols = [col for col in feature_cols if col not in df_normal.columns]\n",
    "if missing_cols:\n",
    "    print(f\"CRITICAL WARNING: Missing feature columns in DataFrame that are expected: {missing_cols}\")\n",
    "    print(f\"This indicates an issue in stream_data.py or event_consumer.py. Please resolve before proceeding for best results.\")\n",
    "    # Filter feature_cols to only include those present to avoid errors.\n",
    "    feature_cols = [col for col in feature_cols if col in df_normal.columns]\n",
    "    print(f\"Proceeding with available feature columns: {feature_cols}\")\n",
    "\n",
    "X_train_raw = df_normal[feature_cols].copy()\n",
    "\n",
    "# Handle any potential NaNs (though with correct streaming, this should be minimal for NASA data)\n",
    "# For this project, we'll drop rows with NaNs. For production, imputation or more robust handling is preferred.\n",
    "if X_train_raw.isnull().sum().sum() > 0:\n",
    "    print(f\"WARNING: Found {X_train_raw.isnull().sum().sum()} NaN values in features. Dropping rows with NaNs.\")\n",
    "    X_train_raw.dropna(inplace=True)\n",
    "    print(f\"New training features DataFrame shape after dropping NaNs: {X_train_raw.shape}\")\n",
    "\n",
    "print(f\"Raw training features DataFrame shape: {X_train_raw.shape}\")\n",
    "print(\"\\nDescriptive statistics for raw training features:\")\n",
    "display(X_train_raw.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c1426-571c-4c12-921b-b48ec9041af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Data Scaling ---\n",
    "# Initialize and fit the scaler on your raw training features.\n",
    "# MinMaxScaler is suitable for anomaly detection as it preserves zero anomalies.\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "\n",
    "print(\"Training data scaled successfully!\")\n",
    "print(\"Shape of scaled data:\", X_train_scaled.shape)\n",
    "print(\"\\nFirst 5 rows of scaled data:\")\n",
    "print(X_train_scaled[:5])\n",
    "\n",
    "# Store the feature column names used for scaling in the correct order.\n",
    "# This list is absolutely CRUCIAL for real-time inference, as incoming data\n",
    "# must be ordered identically before scaling and prediction.\n",
    "scaled_feature_names = X_train_raw.columns.tolist() \n",
    "print(f\"\\nFeature names used for scaling (order matters): {scaled_feature_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be4c62-373c-4486-8e39-b724efd0ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Anomaly Detection Model Training & Evaluation ---\n",
    "print(\"Initializing Isolation Forest model...\")\n",
    "# Isolation Forest is a good choice for unsupervised anomaly detection:\n",
    "# - n_estimators: Number of isolation trees to build.\n",
    "# - contamination: The expected proportion of outliers in the data. This helps the model\n",
    "#                  determine its internal decision threshold. For training on primarily\n",
    "#                  'normal' data, setting it to a small value (e.g., 0.01 for 1%) is common.\n",
    "# - random_state: For reproducibility of results.\n",
    "model = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "\n",
    "print(\"Training Isolation Forest model on scaled normal data (X_train_scaled)...\")\n",
    "# Fit the model. It learns what 'normal' data looks like.\n",
    "model.fit(X_train_scaled)\n",
    "\n",
    "print(\"Isolation Forest model trained successfully!\")\n",
    "\n",
    "# Predict anomalies (-1 for outlier, 1 for inlier) on the training data itself\n",
    "predictions = model.predict(X_train_scaled)\n",
    "anomaly_scores = model.decision_function(X_train_scaled) # Raw anomaly score (lower/more negative is more anomalous)\n",
    "\n",
    "print(\"\\nPredictions (first 20):\", predictions[:20])\n",
    "print(\"Anomaly Scores (first 20):\", anomaly_scores[:20])\n",
    "\n",
    "# Analyze the distribution of anomaly scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(anomaly_scores, bins=50, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Anomaly Scores for Training Data')\n",
    "plt.xlabel('Anomaly Score (lower is more anomalous)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.axvline(x=0, color='red', linestyle='--', label='Default Decision Boundary (score < 0 is anomaly)')\n",
    "plt.axvline(x=model.offset_, color='green', linestyle=':', label=f'Contamination Threshold ({model.contamination*100}%): {model.offset_:.4f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Minimum anomaly score: {anomaly_scores.min():.4f}\")\n",
    "print(f\"Maximum anomaly score: {anomaly_scores.max():.4f}\")\n",
    "print(f\"Mean anomaly score: {anomaly_scores.mean():.4f}\")\n",
    "\n",
    "# The model's internal threshold is set based on the 'contamination' parameter.\n",
    "# model.threshold_ is the raw score below which data is considered anomalous.\n",
    "# model.offset_ is the value used to determine the `predict` output (-1 or 1).\n",
    "print(f\"\\nModel's internal threshold (derived from contamination={model.contamination}): {model.offset_:.4f} (scores below this are classified as anomalies)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb07624-6443-4990-89ef-be19d957a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Cell 7: Save Model and Scaler ---\n",
    "# Define a directory to save your model artifacts\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "model_path = os.path.join(MODEL_DIR, \"anomaly_model.pkl\")\n",
    "scaler_path = os.path.join(MODEL_DIR, \"scaler.pkl\")\n",
    "scaled_feature_names_path = os.path.join(MODEL_DIR, \"scaled_feature_names.json\")\n",
    "\n",
    "# Save the trained model using joblib (recommended for scikit-learn models)\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Anomaly detection model saved to: {model_path}\")\n",
    "\n",
    "# Save the fitted scaler. This is ESSENTIAL because new, incoming data\n",
    "# for real-time inference MUST be scaled using the *exact same scaler*\n",
    "# that was trained on your normal data.\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save the list of feature names and their order. This is CRUCIAL for inference\n",
    "# to ensure that incoming data columns are processed in the same order the\n",
    "# model and scaler were trained on.\n",
    "with open(scaled_feature_names_path, 'w') as f:\n",
    "    json.dump(scaled_feature_names, f)\n",
    "print(f\"Scaled feature names saved to: {scaled_feature_names_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8ccfb-e046-48a8-a4d0-18ba6d3ea4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: Azure Machine Learning Model Deployment ---\n",
    "# Replace placeholders with your actual Azure Subscription ID and AML Workspace Name.\n",
    "# Your Resource Group is \"iot-anomaly-rg\".\n",
    "\n",
    "# 1. Connect to your Azure ML Workspace\n",
    "# It will first try to connect using config.json (generated by az ml workspace show).\n",
    "# If that fails, it falls back to interactive login.\n",
    "try:\n",
    "    ws = Workspace.from_config() \n",
    "    print(\"Connected to Azure ML Workspace using config.json.\")\n",
    "except Exception:\n",
    "    print(\"config.json not found or failed to connect. Authenticating directly...\")\n",
    "    \n",
    "    # --- REPLACE THESE PLACEHOLDERS WITH YOUR ACTUAL AZURE ML WORKSPACE DETAILS ---\n",
    "    subscription_id = \"<Your_Subscription_Id>\" # From Azure Portal -> AML Workspace Overview\n",
    "    resource_group = \"iot-anomaly-rg\"        # Your resource group name\n",
    "    workspace_name = \"praveenchalla-iot-aml\" # Your Azure ML Workspace name\n",
    "\n",
    "    # Interactive login will open a browser window for authentication\n",
    "    auth = InteractiveLoginAuthentication() \n",
    "    ws = Workspace(subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   workspace_name=workspace_name,\n",
    "                   auth=auth)\n",
    "    print(f\"Connected to Azure ML Workspace '{workspace_name}' directly.\")\n",
    "    \n",
    "# --- IMPORTANT: Explicitly write config.json to the current directory ---\n",
    "# This ensures that config.json is always created/updated in your project folder\n",
    "# after a successful connection, making future runs smoother.\n",
    "ws.write_config() \n",
    "print(\"config.json written to local directory for future use.\")\n",
    "\n",
    "\n",
    "# 2. Register the Model (including scaler and feature names)\n",
    "model_name = \"iot-anomaly-model\" \n",
    "\n",
    "# Register the model and scaler files saved locally in the 'models' directory.\n",
    "# Azure ML will create a unique version if you re-register with the same name.\n",
    "model_registration = Model.register(workspace=ws,\n",
    "                                    model_path=MODEL_DIR, # Points to the local directory containing model artifacts\n",
    "                                    model_name=model_name,\n",
    "                                    description=\"Isolation Forest Anomaly detection model for IoT turbofan engine sensors (NASA FD001)\",\n",
    "                                    tags={'area': 'predictive_maintenance', 'model_type': 'anomaly_detection', 'dataset': 'NASA_FD001'},\n",
    "                                    model_framework=Model.Framework.SCIKITLEARN, # Use enum for framework\n",
    "                                    model_framework_version=str(sklearn.__version__)) # Convert version to string\n",
    "\n",
    "print(f\"Model '{model_name}' registered successfully. Version: {model_registration.version}\")\n",
    "\n",
    "\n",
    "# 3. Create an Environment (defines Python dependencies for the deployed service)\n",
    "# This environment is built into a Docker image by Azure ML.\n",
    "env_name = \"anomaly-detection-env\"\n",
    "# Environment.from_conda_specification uses your conda_env.yml file.\n",
    "# This file must be in the same directory as this notebook.\n",
    "env = Environment.from_conda_specification(name=env_name, file_path=\"conda_env.yml\")\n",
    "env.python.user_managed_dependencies = False # Let AML manage dependencies within the image\n",
    "env.docker.enabled = True # Ensure Docker is enabled for the environment\n",
    "\n",
    "print(f\"Environment '{env_name}' created from conda_env.yml.\")\n",
    "\n",
    "\n",
    "# 4. Create an Inference Configuration (connects entry script and environment)\n",
    "# The 'entry_script' is score.py, which defines init() and run().\n",
    "# This file must be in the same directory as this notebook.\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\",\n",
    "                                   environment=env)\n",
    "print(\"Inference configuration created.\")\n",
    "\n",
    "\n",
    "# 5. Define Deployment Configuration (specify compute type and resources for the endpoint)\n",
    "# AciWebservice is suitable for quick, low-cost deployment for development/demo.\n",
    "# For production, AksWebservice is used for higher scale/reliability.\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                                memory_gb=1.5, # Increased memory slightly for robustness\n",
    "                                                enable_app_insights=True, # Good for monitoring logs from deployed service\n",
    "                                                auth_enabled=False # Set to True for production with API Key\n",
    "                                                )\n",
    "print(\"ACI deployment configuration created.\")\n",
    "\n",
    "\n",
    "# 6. Deploy the Model as a Web Service (Real-time Endpoint)\n",
    "service_name = \"iot-anomaly-service\"\n",
    "# The 'overwrite=True' allows you to re-run this cell to update the service\n",
    "# if it already exists with the same name, without manual deletion first.\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model_registration], # Use the registered model version\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aci_config,\n",
    "                       overwrite=True) \n",
    "\n",
    "print(f\"\\nDeploying service '{service_name}'...\")\n",
    "service.wait_for_deployment(show_output=True) # Wait for deployment to complete and show logs in notebook\n",
    "\n",
    "print(f\"\\n--- Model Deployed Successfully! ---\")\n",
    "print(f\"Scoring URI: {service.scoring_uri}\")\n",
    "print(f\"Service state: {service.state}\")\n",
    "\n",
    "# If you set auth_enabled=True in aci_config, retrieve API key:\n",
    "if aci_config.auth_enabled:\n",
    "    api_key = service.get_keys()[0] # Get the primary key (index 0)\n",
    "    print(f\"API Key: {api_key}\")\n",
    "else:\n",
    "    api_key = \"N/A (Auth not enabled)\" # Placeholder if auth is off\n",
    "\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(\"1. Copy the 'Scoring URI' above.\")\n",
    "print(\"2. If authentication was enabled, copy the 'API Key'.\")\n",
    "print(\"3. Update your .env file in your main 'iot-anomaly-project' directory with:\")\n",
    "print(f\"   ML_ENDPOINT_URL=\\\"{service.scoring_uri}\\\"\")\n",
    "print(f\"   ML_ENDPOINT_KEY=\\\"{api_key}\\\"\") \n",
    "\n",
    "print(\"\\n4. Proceed to Phase 3.5: Integrate Deployed Model into event_consumer.py\")\n",
    "print(\"   (You'll open event_consumer.py and modify it to call this API endpoint.)\")\n",
    "\n",
    "\n",
    "# --- Optional Cell for Cleaning Up Deployed Endpoint (CRUCIAL FOR COST SAVINGS!) ---\n",
    "# You MUST delete your ACI endpoint when not in use if you want to stay within your Azure credit budget.\n",
    "# A deployed endpoint incurs costs even when idle.\n",
    "# Run this cell ONLY when you are finished testing and want to delete the deployed service!\n",
    "# service_to_delete_name = \"iot-anomaly-service\" # Ensure this matches your service name\n",
    "# try:\n",
    "#     service_to_delete = Webservice(workspace=ws, name=service_to_delete_name)\n",
    "#     service_to_delete.delete() \n",
    "#     print(f\"Service '{service_to_delete_name}' deleted.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting service {service_to_delete_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
